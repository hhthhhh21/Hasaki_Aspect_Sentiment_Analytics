{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import libraby\n",
    "import random\n",
    "import pandas as pd\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.by import By\n",
    "from time import sleep\n",
    "from datetime import datetime\n",
    "import os\n",
    "\n",
    "# Folder path\n",
    "folder_path = \"D:/hasaki_crawling/data\"\n",
    "\n",
    "# Add user agent\n",
    "user_agent = \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/121.0.0.0 Safari/537.36\"\n",
    "\n",
    "# Setting Options\n",
    "options = webdriver.ChromeOptions()\n",
    "options.add_argument(f\"user-agent={user_agent}\")\n",
    "options.add_argument(\"--ignore-certificate-errors\")\n",
    "options.add_argument(\"--start-maximized\")\n",
    "options.add_argument(\"--disable-popup-blocking\")\n",
    "options.add_argument(\"--no-sandbox\")\n",
    "\n",
    "# Get review counts:\n",
    "def get_reviews_counts(string):\n",
    "    start_index = string.find('(')\n",
    "    end_index = string.find(')')\n",
    "    review_count = string[start_index + 1:end_index]\n",
    "    return review_count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scraped IDs from page 1\n",
      "Scraped IDs from page 2\n",
      "Scraped IDs from page 3\n",
      "Scraped IDs from page 4\n",
      "Scraped IDs from page 5\n",
      "Scraped IDs from page 6\n",
      "Scraped IDs from page 7\n",
      "Scraped IDs from page 8\n",
      "Scraped IDs from page 9\n",
      "Scraped IDs from page 10\n",
      "Scraped IDs from page 11\n",
      "Scraped IDs from page 12\n",
      "Scraped IDs from page 13\n",
      "Scraped IDs from page 14\n",
      "Scraped IDs from page 15\n",
      "Scraped IDs from page 16\n",
      "Scraped IDs from page 17\n",
      "Scraped IDs from page 18\n",
      "Scraped IDs from page 19\n",
      "Scraped IDs from page 20\n",
      "Scraped IDs from page 21\n",
      "Scraped IDs from page 22\n",
      "Scraped IDs from page 23\n",
      "Scraped IDs from page 24\n",
      "Scraped IDs from page 25\n",
      "Scraped IDs from page 26\n",
      "Scraped IDs from page 27\n",
      "Scraped IDs from page 28\n",
      "Scraped IDs from page 29\n",
      "Scraped IDs from page 30\n",
      "Scraped IDs from page 31\n",
      "Scraped IDs from page 32\n",
      "Scraped IDs from page 33\n",
      "Scraped IDs from page 34\n",
      "Scraped IDs from page 35\n",
      "Scraped IDs from page 36\n",
      "Scraped IDs from page 37\n",
      "Scraped IDs from page 38\n",
      "Scraped IDs from page 39\n",
      "Scraped IDs from page 40\n"
     ]
    }
   ],
   "source": [
    "# Initialize browsser\n",
    "driver = webdriver.Chrome(options= options)\n",
    "\n",
    "# URL to crawl\n",
    "start_url = \"https://hasaki.vn/danh-muc/my-pham-high-end-c1907.html\"\n",
    "\n",
    "# GET INFOMATION OF ALL ITEMS\n",
    "df_list = []\n",
    "\n",
    "for page in range(1, 40+1):\n",
    "    \n",
    "    product_ids, data_product_ids, links, title = [], [], [], []\n",
    "    price, discount, review_count, quantity_sold, product_variant = [], [], [], [], []\n",
    "\n",
    "    # Get url\n",
    "    url = f\"{start_url}?p={page}\"\n",
    "    driver.get(url)\n",
    "    sleep(random.randint(7,8))\n",
    "\n",
    "    # Get product-id, data-product-id, link\n",
    "    elems_link_id = driver.find_elements(By.CSS_SELECTOR, '.ProductGrid__grid .block_info_item_sp')\n",
    "    product_ids = [elem.get_attribute('data-id') for elem in elems_link_id] + product_ids\n",
    "    data_product_ids = [elem.get_attribute('data-product') for elem in elems_link_id] + data_product_ids\n",
    "    links = [elem.get_attribute('href') for elem in elems_link_id] + links\n",
    "    sleep(random.randint(1,2))\n",
    "\n",
    "    # Get title\n",
    "    elems_title = driver.find_elements(By.CSS_SELECTOR, '.ProductGrid__grid .vn_names')\n",
    "    title = [elem.text for elem in elems_title] + title\n",
    "    sleep(random.randint(1,2))\n",
    "\n",
    "    # Get price\n",
    "    elems_price = driver.find_elements(By.CSS_SELECTOR, '.ProductGrid__grid .item_giacu')\n",
    "    price = [elem.text for elem in elems_price] + price  \n",
    "    sleep(random.randint(1,2))\n",
    "\n",
    "    # Get discount\n",
    "    elems_discount = driver.find_elements(By.CSS_SELECTOR, '.ProductGrid__grid .item_giamoi')\n",
    "    discount = [elem.text for elem in elems_discount] + discount\n",
    "    sleep(random.randint(1,2))\n",
    "\n",
    "    # Get review counts:\n",
    "    elems_review_count = driver.find_elements(By.CSS_SELECTOR, '.ProductGrid__grid .block_count_by')\n",
    "    review_count = [get_reviews_counts(elem.text) for elem  in elems_review_count] + review_count\n",
    "    sleep(random.randint(1,2))\n",
    "\n",
    "    # Get quantity_sold\n",
    "    elems_quantity_sold = driver.find_elements(By.CSS_SELECTOR, '.ProductGrid__grid .item_count_by')\n",
    "    quantity_sold = [elem.text.strip('\\n ') for elem  in elems_quantity_sold] + quantity_sold\n",
    "    sleep(random.randint(1,2))\n",
    "\n",
    "    # Get product_variant\n",
    "    elems_product_variant = driver.find_elements(By.CSS_SELECTOR, \".ProductGrid__grid .block_info_item_sp\")\n",
    "    product_variant = [elem.get_attribute('data-variant') for elem  in elems_product_variant] + product_variant\n",
    "    sleep(random.randint(1,2))\n",
    "\n",
    "    print(f\"Scraped IDs from page {page}\")\n",
    "    sleep(random.randint(7,8))\n",
    "\n",
    "    # Create dataframe for data crawled\n",
    "    product_data = pd.DataFrame(\n",
    "        list(zip(product_ids, data_product_ids, links, title,\n",
    "            price, discount, quantity_sold, product_variant, review_count\n",
    "        )),\n",
    "        columns=[\n",
    "            'product_id', 'data_product_id', 'link_item', 'title', \n",
    "            'original_price', 'current_price', 'quantity_sold', 'product_variant','review_count'\n",
    "        ]\n",
    "    )\n",
    "\n",
    "    df_list.append(product_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Combine all comment crawled\n",
    "combined_product_data = pd.concat(df_list, ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All product IDs saved to CSV file.\n"
     ]
    }
   ],
   "source": [
    "# Save into csv\n",
    "current_datetime = datetime.now().strftime(\"%Y%m%d_%H%M\")\n",
    "productdata_filename = f\"productdata_{current_datetime}.csv\"\n",
    "\n",
    "combined_product_data.to_csv(os.path.join(folder_path, \"product\", productdata_filename), encoding= \"utf-8-sig\")\n",
    "print(\"All product IDs saved to CSV file.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
